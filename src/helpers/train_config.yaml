# config.yaml
project:
  name: "TeleQaFinetune"
  experiment_version: "1.0.0"

data:
  train_path_1: "data/TeleQnA_training.txt"
  train_path_2: "data/Q_A_ID_training.csv"
  rag_file_path: "data/train_with_rag_256.csv"
  index_dir: "indexes"
  index_name: "teleqa_256_faiss_openai_bge_base_v1.5"
  test_size: 0.1
  random_state: 42

model:
  name: "microsoft/phi-2"
  load_in_8bit: true
  torch_dtype: "float32"

lora:
  r: 64
  lora_alpha: 16
  target_modules:
    - "Wqkv"
    - "fc1"
    - "fc2"
  bias: "none"
  lora_dropout: 0.05

training:
  steps_save_eval_loss: 50
  number_step_partitions: 40
  warmup_steps: 250
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2
  learning_rate: 2.5e-5
  optim: "paged_adamw_8bit"

wandb:
  enabled: true
  project: "TeleQaFinetune"